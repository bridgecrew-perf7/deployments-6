<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
{% if hadoop_ha_enabled %}
  <property>
    <name>dfs.nameservices</name>
    <value>{{ hadoop_nameservices }}</value>
  </property>
  <property>
    <name>dfs.ha.namenodes.{{ hadoop_nameservices }}</name>
    <value>{% for host in hadoop_namenodes %}{% if loop.index > 1%},{% endif %}{{ hostvars[host]['ansible_hostname'] }}{% endfor %}</value>
  </property>
  {% for host in hadoop_namenodes -%}
  <property>
    <name>dfs.namenode.rpc-address.{{ hadoop_nameservices }}.{{ hostvars[host]['ansible_hostname'] }}</name>
    <value>{{ hostvars[host]['ansible_hostname'] }}:8020</value>
  </property>
  {% endfor -%}
  {% for host in hadoop_namenodes -%}
  <property>
    <name>dfs.namenode.http-address.{{ hadoop_nameservices }}.{{ hostvars[host]['ansible_hostname'] }}</name>
    <value>{{ hostvars[host]['ansible_hostname'] }}:50070</value>
  </property>
  {% endfor -%}
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://{{ hadoop_journalnodes | join(':8485' + ';') }}:8485/{{ hadoop_nameservices }}</value>
  </property>
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>{{ hadoop_dfs_journalnode_edits_dir }}</value>
  </property>
  <property>
    <name>dfs.client.failover.proxy.provider.{{ hadoop_nameservices }}</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  {% if hadoop_ssh_fence -%}
  <property>
    <name>dfs.ha.fencing.methods</name>
    <value>sshfence</value>
  </property>
  <property>
    <name>dfs.ha.fencing.ssh.private-key-files</name>
    <value>{{ hadoop_user_home }}/.ssh/id_rsa</value>
  </property>
  {% else -%}
  <property>
      <name>dfs.ha.fencing.methods</name>
      <value>shell(/bin/true)</value>
  </property>
  {% endif -%}
  <property>
    <name>dfs.ha.automatic-failover.enabled</name>
    <value>true</value>
  </property>
{% endif %}
  <property>
    <name>dfs.replication</name>
    <value>{{ hadoop_dfs_replication }}</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>{{ hadoop_datanode_dir_list | map('regex_replace', '^(.*)$', 'file://\\1' ) | join(',') }}</value>
  </property>
{% if inventory_hostname in hadoop_namenodes %}
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>{{ hadoop_namenode_dir_list | map('regex_replace', '^(.*)$', 'file://\\1' ) | join(',') }}</value>
  </property>
{% endif %}
  <property>
    <name>dfs.permissions.superusergroup</name>
    <value>{{ hadoop_dfs_permissions_superusergroup }}</value>
  </property>
  <property>
    <name>fs.permissions.umask-mode</name>
    <value>{{ hadoop_fs_permissions_umask_mode }}</value>
  </property>
  <property>
    <name>dfs.hosts.exclude</name>
    <value>{{ hadoop_conf_dir }}/dfs.hosts.exclude</value>
  </property>
  <property>
    <name>dfs.blocksize</name>
    <value>{{ hadoop_dfs_blocksize }}</value>
    <final>true</final>
  </property>
  <property>
    <name>dfs.namenode.avoid.read.stale.datanode</name>
    <value>{{ hadoop_dfs_namenode_avoid_read_stale_datanode | lower }}</value>
  </property>
  <property>
    <name>dfs.namenode.avoid.write.stale.datanode</name>
    <value>{{ hadoop_dfs_namenode_avoid_write_stale_datanode | lower }}</value>
  </property>
  <property>
    <name>dfs.support.append</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.namenode.write.stale.datanode.ratio</name>
    <value>{{ hadoop_dfs_namenode_write_stale_datanode_ratio }}</value>
  </property>
  <property>
    <name>dfs.namenode.handler.count</name>
    <value>{{ hadoop_dfs_namenode_handler_count }}</value>
  </property>
  <property>
    <name>dfs.namenode.service.handler.count</name>
    <value>{{ hadoop_dfs_namenode_service_handler_count }}</value>
  </property>
  <property>
    <name>dfs.datanode.du.reserved</name>
    <value>{{ hadoop_dfs_datanode_du_reserved }}</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir.perm</name>
    <value>{{ hadoop_dfs_datanode_data_dir_perm }}</value>
  </property>
  <property>
    <name>dfs.datanode.max.transfer.threads</name>
    <value>{{ hadoop_dfs_datanode_max_transfer_threads }}</value>
  </property>
  <property>
    <name>dfs.datanode.fsdataset.volume.choosing.policy</name>
    <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
  </property>
  <property>
    <name>dfs.replication.max</name>
    <value>{{ hadoop_dfs_replication_max }}</value>
  </property>
  <property>
    <name>dfs.namenode.replication.min</name>
    <value>{{ hadoop_dfs_namenode_replication_min }}</value>
  </property>
  <property>
    <name>dfs.namenode.checkpoint.period</name>
    <value>{{ hadoop_dfs_namenode_checkpoint_period }}</value>
  </property>
  <property>
    <name>dfs.namenode.audit.log.async</name>
    <value>{{ hadoop_dfs_namenode_audit_log_async | lower }}</value>
  </property>
  <property>
    <name>dfs.client.file-block-storage-locations.num-threads</name>
    <value>{{ hadoop_dfs_client_file_block_storage_locations_num_threads }}</value>
  </property>
  <property>
    <name>dfs.client.file-block-storage-locations.timeout.millis</name>
    <value>{{ hadoop_dfs_client_file_block_storage_locations_timeout_millis }}</value>
  </property>
  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>{{ hadoop_enable_short_circuit_reads | lower }}</value>
  </property>
{% if hadoop_enable_short_circuit_reads is defined %}
  <property>
    <name>dfs.domain.socket.path</name>
    <value>{{ hadoop_dfs_domain_socket_path_folder }}/dn._PORT</value>
  </property>
{% endif %}
</configuration>