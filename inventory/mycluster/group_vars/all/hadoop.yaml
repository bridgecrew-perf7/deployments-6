# ------------------------------------------------------------------------------
# Hadoop configuration
# ------------------------------------------------------------------------------
## hadoop's configuration will define here.
hadoop_install_dir: "{{ datalandfill_root_dir }}/hadoop"
hadoop_log_dir: "{{ hadoop_install_dir }}/log"
hadoop_conf_dir: "{{hadoop_install_dir}}/etc/hadoop"
hadoop_bin_dir: "{{hadoop_install_dir}}/bin"
hadoop_tmp_dir: /tmp/hadoop
hadoop_data_dir: "/data01, /data02, /data03"

# Symlink for hadoop to the version you are installing
hadoop_conf_symlink_dir: /etc/hadoop/conf

# Directories where namenode should store metadata
hadoop_namenode_dir_list:
  - "{{datalandfill_data_dir}}/dfs/namenode"

# Directories where secondary namenode should store temporary images to merge
hadoop_namenode_checkpoint_dir_list:
  - "{{datalandfill_data_dir}}/dfs/secondarynamenode"

# Directories where datanodes should store data
hadoop_datanode_dir_list:
  - "{{datalandfill_data_dir}}/dfs/datanode"

# Directories where journal nodes should store edits
hadoop_dfs_journalnode_edits_dir: "{{datalandfill_data_dir}}/dfs/journaldirnode"

hadoop_dfs_journalnode_edits_dir_perm: "700"
hadoop_nodemanager_local_dir: "{{datalandfill_data_dir}}/nodemanager/local"
hadoop_nodemanager_log_dir: "{{datalandfill_data_dir}}/nodemanager/container-logs"
# ------------------------------------------------------------------------------
# General cluster settings
# ------------------------------------------------------------------------------
hadoop_cluster_name: hadoop-cluster
hadoop_user: hdfs
hadoop_user_home: "/home/{{ hadoop_user }}"
hadoop_upgrade: False
hadoop_upgrade_force: False
hadoop_extra_classpath: []
hadoop_ssh_known_hosts_file: "{{ hadoop_user_home }}/.ssh/known_hosts"


hadoop_enable_short_circuit_reads: true  # IMPORTANT: this property should be 'true' or 'false'

# ------------------------------------------------------------------------------
# Hadoop host variables
# ------------------------------------------------------------------------------
hadoop_namenodes: "{{ groups.hadoop_master_node }}"
hadoop_secondary_namenode: "{{ groups.secondarynamenode if groups.secondarynamenode is defined else [] }}"
hadoop_hadoop_hosts: "{{ groups.hadoop_hosts }}"
hadoop_journalnodes: "{{ groups.hadoop_journal_node }}"
hadoop_datanodes: "{{ groups.hadoop_data_node }}"
hadoop_zookeeper_hosts: "{{ groups.zookeeper_node }}"
# resource manager
hadoop_nodemanagers: "{{ groups.nodemanager_node }}"
hadoop_resourcemanagers: "{{ groups.resourcemanager_node }}"
hadoop_jobhistorys: "{{ groups.jobhistory }}"

# hadoop exclude hosts
hadoop_decommission: false
hadoop_dfs_hosts_exclude: false
hadoop_yarn_hosts_exclude: false
# yarn cluster id
hadoop_yarn_cluster_id: yarn-cluster

# Bootstraps the cluster ( Format namenodes, zkfc, journalnodes, start all services)
# Please read the code before you activate this option.
# Especially if you already have a hadoop setup in place.
hadoop_bootstrap: True
# Use ansible handlers?
hadoop_ansible_handlers: True
# Redistribute ssh keys every time?
hadoop_redistribute_ssh_keys: True
# ------------------------------------------------------------------------------
# Hadoop installation source
# ------------------------------------------------------------------------------
hadoop_distribution_method: "download"   # Method to use for archive installation ("download", "local_file" or "compile")
hadoop_download_url: "{{datalandfill_repo_url}}/hadoop/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz"

# ------------------------------------------------------------------------------
# HA specific setup
# ------------------------------------------------------------------------------
# Use ssh as fencing method (other option is shell(/bin/true)
hadoop_ssh_fence: True

hadoop_ha_enabled: "{{hadoop_namenodes | count > 1}}"
hadoop_default_fs: "hdfs://{{ hadoop_nameservices if hadoop_ha_enabled else hadoop_namenodes[0] + ':8020' }}"
hadoop_nameservices: "{{ hadoop_cluster_name }}"
hadoop_zookeeper_client_port: 2181
hadoop_zookeeper_quorum: "{{ groups['zookeeper_node'] | join(':' + (hadoop_zookeeper_client_port | string) + ',')  }}:{{ hadoop_zookeeper_client_port | string }}"

# ------------------------------------------------------------------------------
# Extended core-site.xml
# ------------------------------------------------------------------------------
hadoop_tmp_dir_user: "{{hadoop_tmp_dir}}/hadoop-${user.name}"
# hadoop_tmp_dir_user: "{{hadoop_tmp_dir}}/hadoop-tmp"
hadoop_fs_trash_interval: 0
hadoop_fs_trash_checkpoint_interval: 0  # If 0 this is set to the value of hadoop_fs_trash_interval by hadoop

# ------------------------------------------------------------------------------
# Extended hdfs-site.xml
# ------------------------------------------------------------------------------
hadoop_fs_permissions_umask_mode: "002"
hadoop_dfs_permissions_superusergroup: "{{hadoop_group}}"
hadoop_dfs_blocksize: 134217728
hadoop_dfs_namenode_write_stale_datanode_ratio: "0.5f"
hadoop_dfs_datanode_du_reserved: "1073741824"
hadoop_dfs_datanode_data_dir_perm: "700"
hadoop_dfs_datanode_max_transfer_threads: 4096
hadoop_dfs_replication: 3
hadoop_dfs_replication_max: 50
hadoop_dfs_namenode_replication_min: 1
hadoop_dfs_namenode_checkpoint_period: 3600
# the recommended 'namenode handler count' is best defined by formula: lb(#datanodes) * 20
# and recommended 'service handler count'  50% of the previous value
# Ref: https://community.hortonworks.com/articles/43839/scaling-the-hdfs-namenode-part-2.html
# -> for an average cluster 10-20 DNs the value 64 is a good average (for 32+ DNs -> 100+)
hadoop_dfs_namenode_handler_count: 32
hadoop_dfs_namenode_service_handler_count: "{{ (hadoop_dfs_namenode_handler_count / 2)|int}}"
hadoop_dfs_namenode_avoid_read_stale_datanode: true
hadoop_dfs_namenode_avoid_write_stale_datanode: true
hadoop_dfs_namenode_audit_log_async: false
hadoop_dfs_client_file_block_storage_locations_num_threads: 10
hadoop_dfs_client_file_block_storage_locations_timeout_millis: 1000
hadoop_dfs_domain_socket_path_folder: "{{hadoop_install_dir}}"
# ------------------------------------------------------------------------------
# log4j.properties vars
# ------------------------------------------------------------------------------

hadoop_log_maxfilesize: "256MB"
hadoop_log_maxbackupindex: 20

# ------------------------------------------------------------------------------
# hadoop-env.sh vars
# ------------------------------------------------------------------------------
hadoop_namenode_heap_size: "2048m"
hadoop_namenode_javaOpts: "-Xmx{{hadoop_namenode_heap_size}}"
hadoop_datanode_javaOpts: ""

# default logger selection used in hadoop-env.sh
hadoop_security_logger: "INFO,RFAS"
hadoop_audit_logger: "INFO,NullAppender"

# ------------------------------------------------------------------------------
# Rack specific
# ------------------------------------------------------------------------------
hadoop_rackaware: true
hadoop_rack_script_path: "{{hadoop_conf_dir}}/rack-topology.sh"

# hdfs_rack_script_awk: '"{if ($4 < 3) print "rack-1"; else print "rack-2" }"'
# hadoop_rack_script_path: "{{hadoop_conf_dir}}/rack-awareness.sh"
# ------------------------------------------------------------------------------
# Custom scripts
# ------------------------------------------------------------------------------
hadoop_audit_rotate_days: 90 # ISO 27001 compliance
resource_manager_port: 8088

hadoop_hdfs_site: {}
hadoop_hdfs_site_defaults:
  dfs.datanode.balance.max.concurrent.moves: 4
  dfs.datanode.balance.bandwidthPerSec: 2147483648
  dfs.datanode.block-pinning.enabled: false
  dfs.balancer.moverThreads: 20000
  dfs.balancer.max-size-to-move: 107374182400
  dfs.balancer.getBlocks.size:
  dfs.balancer.getBlocks.min-block-size: 104857600

# ------------------------------------------------------------------------------
# Yarn site variables
# ------------------------------------------------------------------------------
# yarn-site.xml
hadoop_yarn_site: {}
hadoop_yarn_site_defaults:
  # acl
  yarn.acl.enable: false
  yarn.admin.acl: "*"
  yarn.log-aggregation-enable: true
  yarn.log-aggregation.retain-seconds: 604800
  yarn.log-aggregation.retain-check-interval-seconds: 10800

  # yarn scheduler
  #yarn.resourcemanager.scheduler.class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler
  yarn.resourcemanager.scheduler.class: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
  yarn.scheduler.minimum-allocation-mb: 1024
  yarn.scheduler.maximum-allocation-mb: 14336
  yarn.scheduler.increment-allocation-mb: 512
  yarn.scheduler.minimum-allocation-vcores:  1
  yarn.scheduler.increment-allocation-vcores:  1
  yarn.scheduler.maximum-allocation-vcores:  60
  yarn.scheduler.capacity.resource-calculator: org.apache.hadoop.yarn.util.resource.DominantResourceCalculator
  yarn.scheduler.fair.allow-undeclared-pools: true
  yarn.scheduler.fair.user-as-default-queue: true
  yarn.scheduler.fair.preemption: false
  yarn.scheduler.fair.preemption.cluster-utilization-threshold: 0.8
  yarn.scheduler.fair.sizebasedweight: false
  yarn.scheduler.fair.assignmultiple: true
  yarn.scheduler.fair.dynamicmaxassign: true
  yarn.scheduler.fair.maxassign: -1
  yarn.scheduler.fair.continuous-scheduling-enabled: false
  yarn.scheduler.fair.locality-delay-node-ms: 2000
  yarn.scheduler.fair.locality-delay-rack-ms: 4000
  yarn.scheduler.fair.continuous-scheduling-sleep-ms: 5
  yarn.resourcemanager.max-completed-applications: 10000

  yarn.resourcemanager.work-preserving-recovery.enabled: true
  yarn.resourcemanager.client.thread-count: 50
  yarn.resourcemanager.scheduler.client.thread-count: 50
  yarn.resourcemanager.admin.client.thread-count: 1
  yarn.resourcemanager.amliveliness-monitor.interval-ms: 1000
  yarn.am.liveness-monitor.expiry-interval-ms: 600000
  yarn.resourcemanager.am.max-attempts: 2
  yarn.resourcemanager.container.liveness-monitor.interval-ms: 600000
  yarn.resourcemanager.nm.liveness-monitor.interval-ms:  1000
  yarn.nm.liveness-monitor.expiry-interval-ms: 600000
  yarn.resourcemanager.resource-tracker.client.thread-count: 50

  # client sleep
  yarn.client.failover-sleep-base-ms: 100
  yarn.client.failover-sleep-max-ms: 2000

  # yarn.nodemanager.vmem-pmem-ratio: 2.1
  yarn.nodemanager.local-dirs: "{{hadoop_nodemanager_local_dir}}"
  yarn.nodemanager.log-dirs: "{{hadoop_nodemanager_log_dir}}"
  yarn.nodemanager.log.retain-seconds: 10800
  yarn.nodemanager.remote-app-log-dir-suffix: logs
  yarn.nodemanager.env-whitelist: JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME
  yarn.nodemanager.recovery.dir: /var/lib/hadoop-yarn/yarn-nm-recovery
  yarn.nodemanager.container-monitor.interval-ms: 3000
  yarn.nodemanager.aux-services: mapreduce_shuffle,spark_shuffle
  yarn.nodemanager.aux-services.mapreduce_shuffle.class: org.apache.hadoop.mapred.ShuffleHandler
  yarn.nodemanager.aux-services.spark_shuffle.class: org.apache.spark.network.yarn.YarnShuffleService
  spark.shuffle.service.port: 7337
  yarn.nodemanager.container-manager.thread-count: 20
  yarn.nodemanager.delete.thread-count: 4
  yarn.resourcemanager.nodemanagers.heartbeat-interval-ms: 100
  yarn.nodemanager.localizer.cache.cleanup.interval-ms: 600000
  yarn.nodemanager.localizer.cache.target-size-mb: 10240
  yarn.app.mapreduce.am.resource.mb: 1024
  yarn.app.mapreduce.am.command-opts: -Xmx768m
  yarn.nodemanager.localizer.client.thread-count: 5
  yarn.nodemanager.localizer.fetch.thread-count: 4
  yarn.nodemanager.remote-app-log-dir: /tmp/logs
  yarn.nodemanager.resource.memory-mb: 14336
  yarn.nodemanager.linux-container-executor.cgroups.strict-resource-usage: false
  yarn.nodemanager.resource.percentage-physical-cpu-limit: 100
  yarn.nodemanager.resource.cpu-vcores: 8
  yarn.nodemanager.delete.debug-delay-sec: 0
  yarn.nodemanager.health-checker.script.path:
  yarn.nodemanager.health-checker.script.opts:
  yarn.nodemanager.disk-health-checker.interval-ms: 120000
  yarn.nodemanager.disk-health-checker.min-free-space-per-disk-mb: 0
  yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage: 90.0
  yarn.nodemanager.disk-health-checker.min-healthy-disks: 0.25
  yarn.nodemanager.vmem-check-enabled: false
  # yarn.log.server.url: "{{ hadoop_namenodes[0] }}:19888"

  # node lable
  yarn.node-labels.enabled:  true
  yarn.node-labels.fs-store.root-dir: "hdfs://{{hadoop_nameservices}}/yarn/node-labels"
# ------------------------------------------------------------------------------
# mapred site variables
# ------------------------------------------------------------------------------
hadoop_mapred_site: {}
hadoop_mapred_site_defaults:
  mapreduce.framework.name: yarn
  mapreduce.map.memory.mb: 1024
  mapreduce.map.java.opts: -Xmx768m
  mapreduce.reduce.memory.mb: 2048
  mapreduce.reduce.java.opts: -Xmx1536m
  mapreduce.jobhistory.intermediate-done-dir: /mr-history/tmp
  mapreduce.jobhistory.done-dir: /mr-history/done
  mapred.hosts.exclude: "{{ hadoop_conf_dir }}/dfs.hosts.exclude"
  mapreduce.jobhistory.address: "{{ hadoop_namenodes[0] }}:10020"
  mapreduce.jobhistory.webapp.address: "{{ hadoop_namenodes[0] }}:19888"
  mapreduce.jobhistory.admin.address: "{{ hadoop_namenodes[0] }}:10033"
  mapreduce.job.split.metainfo.maxsize: 10000000
  mapreduce.job.counters.max: 120
  mapreduce.job.counters.groups.max: 50
  mapreduce.output.fileoutputformat.compress: false
  mapreduce.output.fileoutputformat.compress.type: BLOCK
  mapreduce.output.fileoutputformat.compress.codec: org.apache.hadoop.io.compress.DefaultCodec
  mapreduce.map.output.compress.codec: org.apache.hadoop.io.compress.SnappyCodec
  mapreduce.map.output.compress: true
  zlib.compress.level: DEFAULT_COMPRESSION
  mapreduce.task.io.sort.factor: 64
  mapreduce.map.sort.spill.percent: 0.8
  mapreduce.reduce.shuffle.parallelcopies: 10
  mapreduce.task.timeout: 600000
  mapreduce.client.submit.file.replication: 3
  mapreduce.job.reduces: 216
  mapreduce.task.io.sort.mb: 256
  mapreduce.map.speculative: false
  mapreduce.reduce.speculative: false
  mapreduce.job.reduce.slowstart.completedmaps: 0.8
  mapreduce.map.cpu.vcores: 1
  mapreduce.reduce.cpu.vcores: 1
  mapreduce.job.heap.memory-mb.ratio: 0.8
  mapreduce.job.acl-view-job: "*"
  mapreduce.job.acl-modify-job: 
  mapreduce.cluster.acls.enabled: false